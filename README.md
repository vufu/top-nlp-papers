# Top NLP Papers

In a time where daily new advancements are being made in the field of NLP, keeping up with the latest research can be overwhelming. That's why we've created this Github repository, which aims to features the most impactful and must-read NLP papers. From classic works to cutting-edge research, this list should help us to stay up-to-date with the latest developments.

## How to?
Our repository is all about community-driven knowledge sharing. That's why we rely on [**the Discussion section**](https://github.com/vufu/top-nlp-papers/discussions) to gather, share and vote on the most influential NLP papers. Anyone is welcome to participate in the discussion! Every few months we will update the top 10 papers of a given period according to the voting in the discussion section. 

### Channels
- [**2023 Papers**](https://github.com/vufu/top-nlp-papers/discussions/1)  
- [**2022 Papers**](https://github.com/vufu/top-nlp-papers/discussions/2)  
- [**Classic Papers**](https://github.com/vufu/top-nlp-papers/discussions/3)  


## Papers
### 2022
- **PaLM: Scaling Language Modeling with Pathways** _(Chowdhery et al.)_  
[[Paper]](https://arxiv.org/abs/2204.02311) `LLMs`
- **Chain of Thought Prompting Elicits Reasoning in Large Language Models**  _(Wei et al.)_  
[[Paper]](https://arxiv.org/abs/2201.11903) `Prompting`
- **Training language models to follow instructions with human feedback** _(Ouyang et al.)_  
[[Paper]](https://arxiv.org/abs/2203.02155) `Instruction Tuning`
- **OPT: Open Pre-trained Transformer Language Models** _(Zhang et al.)_  
[[Paper]](https://arxiv.org/abs/2205.01068) [[Github]](https://github.com/facebookresearch/metaseq) `LLMs`
- **LaMDA: Language Models for Dialog Applications** _(Thoppilan et al.)_  
[[Paper]](https://arxiv.org/abs/2201.08239) `LLMs` 
- **Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model** _(Smith et al.)_  
[[Paper]](https://arxiv.org/abs/2201.11990) `LLMs` 
- **Training Compute-Optimal Large Language Model** _(Hoffmann et al.)_  
[[Paper]](https://arxiv.org/abs/2203.15556) `LLMs`  
- **OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework** _(Wang et al.)_  
[[Paper]]() [[Github]](https://github.com/ofa-sys/ofa)`...` 
- **Do As I Can, Not As I Say: Grounding Language in Robotic Affordances** _(Ahn et al.)_  
[[Paper]](https://arxiv.org/abs/2204.01691) [[Website]](https://say-can.github.io/) [[Github]](https://github.com/google-research/google-research/tree/master/saycan) `Dicision Making` `Robot Task Planning` 
- **Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models** _(Srivastava et al.)_  
[[Paper]](https://arxiv.org/abs/2206.04615) [[Github]](https://github.com/google/BIG-bench) `Benchmark` 



### 2021
- **SimCSE: Simple Contrastive Learning of Sentence Embeddings** _(Gao et al.)_  
[[Paper]](https://arxiv.org/abs/2104.08821v4) [[Github]](https://github.com/princeton-nlp/SimCSE) `...`  
- **Prefix-Tuning: Optimizing Continuous Prompts for Generation** _(Lisa et al.)_  
[[Paper]](https://arxiv.org/abs/2101.00190v1) [[Github]](https://github.com/XiangLi1999/PrefixTuning) `...`  
- **Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision** _(Jia et al.)_  
[[Paper]](https://arxiv.org/abs/2102.05918v2) `...`  
- **The Power of Scale for Parameter-Efficient Prompt Tuning** _(Lester et al.)_  
[[Paper]](https://arxiv.org/abs/2104.08691v2) [[Github]](https://github.com/google-research/prompt-tuning) `prompting`  
- **Making Pre-trained Language Models Better Few-shot Learners** _(Gao et al.)_  
[[Paper]](https://arxiv.org/abs/2101.00190v1)  [[Github]](https://github.com/princeton-nlp/LM-BFF) `...`  
- **HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units** _(Hsu et al.)_  
[[Paper]](https://arxiv.org/abs/2106.07447v1) [[HuggingFace]](https://huggingface.co/docs/transformers/model_doc/hubert) `...`  
- **StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery** _(Patshnik et al.)_  
[[Paper]](https://arxiv.org/abs/2103.17249v1) [[Github]](https://github.com/orpatashnik/StyleCLIP) `...`  
- **Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing** _(Liu et al.)_  
[[Paper]](https://arxiv.org/abs/2107.13586v1) `prompting`  
- **Finetuned Language Models Are Zero-Shot Learners** _(Wei et al.)_  
[[Paper]](https://arxiv.org/abs/2109.01652v5) [[Github]](https://github.com/google-research/flan) `Zero-shot` `Instruction Tuning`  
- **Multitask Prompted Training Enables Zero-Shot Task Generalization** _(Sanh et al.)_  
[[Paper]](https://arxiv.org/abs/2110.08207v3) `zero-shot`  

### 2020
- **Language Models are Few-Shot Learners** _(Brown et al.)_  
[[Paper]](https://arxiv.org/abs/2005.14165v4) [[Github]](https://github.com/openai/gpt-3) `GPT-3` `LLMs`  
- **ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators** _(Clark et al.)_  
[[Paper]](https://arxiv.org/abs/2003.10555v1) [[Github]](https://github.com/google-research/electra) `...`  
- **wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations**  _(Baevski et al.)_  
[[Paper]](https://arxiv.org/abs/2006.11477v3) `...`  
- **Longformer: The Long-Document Transformer** _(Beltagy et al.)_  
[[Paper]](https://arxiv.org/abs/2004.05150v2) [[Github]](https://github.com/allenai/longformer) `...`  
- **Dense Passage Retrieval for Open-Domain Question Answering** _(Karpukhi et al.)_  
[[Paper]](https://arxiv.org/abs/2004.04906v3) [[Github]](https://github.com/facebookresearch/DPR) `...`  
- **Don't Stop Pretraining: Adapt Language Models to Domains and Tasks** _(Gururangan et al.)_  
[[Paper]](https://arxiv.org/abs/2004.10964v3) [[Github]](https://github.com/allenai/dont-stop-pretraining) `...`  
- **Reformer: The Efficient Transformer** _(Kitaev et al.)_  
[[Paper]](https://arxiv.org/abs/2001.04451v2) [[Github]](https://github.com/google/trax/tree/master/trax/models/reformer) `...`  
- **Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks** _(Li et al.)_  
[[Paper]](https://arxiv.org/abs/2004.06165v5) [[Github]](https://github.com/microsoft/Oscar) `...`  
- **Big Bird: Transformers for Longer Sequences** _(Zaheer et al.)_  
[[Paper]](https://arxiv.org/abs/2007.14062v2) [[Github]](https://github.com/google-research/bigbird) `...`  
- **Stanza: A Python Natural Language Processing Toolkit for Many Human Languages** _(Qi et al.)_  
[[Paper]](https://arxiv.org/abs/2003.07082v2) [[Github]](https://github.com/stanfordnlp/stanza) `...`  

### 2019
### 2018
### 2017
### 2016
### 2015
### 2010-2014
### 2005-2009
### 2000-2004
### 1990-1999
### 1950-1989
### ~1950

## Tags
- `Factual Verificaiton`
- `Instruction Tuning`
- `LLMs`
- `Relation Extraction`
- `Summarization`
- `Topic Model`

## Other List
- [100 NLP Papers](https://github.com/mhagiwara/100-nlp-papers)  
  **Description:** This is a list of 100 important natural language processing (NLP) papers that serious students and researchers working in the field should probably know about and read. This list is originally based on the answers for a Quora question I posted years ago: [What are the most important research papers which all NLP students should definitely read?](https://www.quora.com/What-are-the-most-important-research-papers-which-all-NLP-students-should-definitely-read-Why)
- [The best NLP papers](https://thebestnlppapers.com/)  
**Description:** This is a list of 100 important natural language processing (NLP) papers that serious students and researchers working in the field should probably know about and read. This list is originally based on the answers for a Quora question I posted years ago: [What are the most important research papers which all NLP students should definitely read?](https://www.quora.com/What-are-the-most-important-research-papers-which-all-NLP-students-should-definitely-read-Why)
